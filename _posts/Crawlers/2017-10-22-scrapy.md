---
layout: life
title: scrapy
category: Crawlers
duoshuo: true
date: 2017-10-22
---

******

	作者: minus
	版本: V 0.0.1
	日期: 2017年10月22日

<!-- more -->

*******

1、 scrapy spider dmoz_spider.py (终端测试)

2、在项目根目录下下执行： scrapy crawl dmoz      #爬虫名字是name属性对应的值
3、在没有项目的情况下，运行某个spider： scrapy runspider myspider.py
http://www.crifan.com/python_run_scrapy_keyerror_spider_not_found/

全局命令:

    startproject
    settings
    runspider
    shell
    fetch
    view
    version 

项目(Project-only)命令:

    crawl
    check
    list
    edit
    parse
    genspider
    deploy
    bench 

命令：genspider(可以列出所有的爬虫模板)
$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

$ scrapy genspider -d basic
import scrapy

class $classname(scrapy.Spider):
    name = "$name"
    allowed_domains = ["$domain"]
    start_urls = (
        'http://www.$domain/',
        )

    def parse(self, response):
        pass

$ scrapy genspider -t basic example example.com
Created spider 'example' using template 'basic' in module:
  mybot.spiders.example

list

    语法: scrapy list
    是否需要项目: yes 

列出当前项目中所有可用的spider。每行输出一个spider。

使用例子:

$ scrapy list
spider1
spider2

crawl

    语法: scrapy crawl <spider>
    是否需要项目: yes 

使用spider进行爬取。

例子:

$ scrapy crawl myspider
[ ... myspider starts crawling ... ]

check

    语法: scrapy check [-l] <spider>
    是否需要项目: yes 

运行contract检查。

例子:

$ scrapy check -l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item

$ scrapy check
[FAILED] first_spider:parse_item
>>> 'RetailPricex' field is missing

[FAILED] first_spider:parse
>>> Returned 92 requests, expected 0..4

fetch （获取html源码）

    语法: scrapy fetch <url>
    是否需要项目: no 

使用Scrapy下载器(downloader)下载给定的URL，并将获取到的内容送到标准输出。

该命令以spider下载页面的方式获取页面。例如，如果spider有 USER_AGENT 属性修改了 User Agent，该命令将会使用该属性。

因此，您可以使用该命令来查看spider如何获取某个特定页面。

该命令如果非项目中运行则会使用默认Scrapy downloader设定。

例子:

$ scrapy fetch --nolog http://www.example.com/some/page.html
[ ... html content here ... ]

$ scrapy fetch --nolog --headers http://www.example.com/
{'Accept-Ranges': ['bytes'],
 'Age': ['1263   '],
 'Connection': ['close     '],
 'Content-Length': ['596'],
 'Content-Type': ['text/html; charset=UTF-8'],
 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],
 'Etag': ['"573c1-254-48c9c87349680"'],
 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],
 'Server': ['Apache/2.2.3 (CentOS)']}

view （用浏览器打开页面观察）

    语法: scrapy view <url>
    是否需要项目: no 

在浏览器中打开给定的URL，并以Scrapy spider获取到的形式展现。 有些时候spider获取到的页面和普通用户看到的并不相同。 因此该命令可以用来检查spider所获取到的页面，并确认这是您所期望的。

例子:

$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]

shell  (可以直接对页面对象response进行操作)

    语法: scrapy shell [url]
    是否需要项目: no 

以给定的URL(如果给出)或者空(没有给出URL)启动Scrapy shell。 查看 Scrapy终端(Scrapy shell) 获取更多信息。

例子:

$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]

parse

    语法: scrapy parse <url> [options]
    是否需要项目: yes 

获取给定的URL并使用相应的spider分析处理。如果您提供 --callback 选项，则使用spider的该方法处理，否则使用 parse 。

支持的选项:

    --spider=SPIDER: 跳过自动检测spider并强制使用特定的spider
    --a NAME=VALUE: 设置spider的参数(可能被重复)
    --callback or -c: spider中用于解析返回(response)的回调函数
    --pipelines: 在pipeline中处理item
    --rules or -r: 使用 CrawlSpider 规则来发现用来解析返回(response)的回调函数
    --noitems: 不显示爬取到的item
    --nolinks: 不显示提取到的链接
    --nocolour: 避免使用pygments对输出着色
    --depth or -d: 指定跟进链接请求的层次数(默认: 1)
    --verbose or -v: 显示每个请求的详细信息

例子:

$ scrapy parse http://www.example.com/ -c parse_item
[ ... scrapy log lines crawling example.com spider ... ]

>>> STATUS DEPTH LEVEL 1 <<<
# Scraped Items  ------------------------------------------------------------
[{'name': u'Example item',
 'category': u'Furniture',
 'length': u'12 cm'}]

# Requests  -----------------------------------------------------------------
[]

settings

    语法: scrapy settings [options]
    是否需要项目: no 

获取Scrapy的设定

在项目中运行时，该命令将会输出项目的设定值，否则输出Scrapy默认设定。

例子:

$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0

runspider

    语法: scrapy runspider <spider_file.py>
    是否需要项目: no 

在未创建项目的情况下，运行一个编写在Python文件中的spider。

例子:

$ scrapy runspider myspider.py
[ ... spider starts crawling ... ]

version

    语法: scrapy version [-v]
    是否需要项目: no 

输出Scrapy版本。配合 -v 运行时，该命令同时输出Python, Twisted以及平台的信息，方便bug提交。
deploy

0.11 新版功能.

    语法: scrapy deploy [ <target:project> | -l <target> | -L ]
    是否需要项目: yes 

将项目部署到Scrapyd服务。查看 部署您的项目 。

Log:

将cmd中信息输入到log文件中：

命令行： scrapy crawl XXX  -s LOG_FILE=xxx.txt

from scrapy import log

log.msg(msg,level=log.INFO)

OR

scrapy.log.start(logfile=None,loglevel=None,logstdout=None)



1、创建项目

1、创建项目
scrapy startproject myproject

2、进入目录
cd myproject   这时候就可以用scrapy 命令来管理和控制你的项目了

3、创建一个新的spider:
scrapy genspider mydomain mydomain.com 
scrapy genspider [-t template] <name> <domain>


Available templates:
  basic
  crawl
  csvfeed
  xmlfeed
